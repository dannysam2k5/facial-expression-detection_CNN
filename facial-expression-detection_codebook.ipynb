{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc78de8",
   "metadata": {},
   "source": [
    "# Facial Expression Detection using Convolutional Neural Networks ü§ñüì∑\n",
    "\n",
    "Facial expression detection is a fascinating field within computer vision that involves the recognition and interpretation of human emotions based on facial expressions. In this Jupyter Notebook, we'll build a Facial Expression Detection model using Convolutional Neural Networks (CNNs). This model will be trained to classify facial expressions into different emotion categories, such as happiness, sadness, anger, etc. üòäüò¢üò†\n",
    "\n",
    "### Goal üöÄüíª\n",
    "The primary goal of this project is to create an effective facial expression detection system that can be integrated into various applications, including human-computer interaction, virtual reality, and emotion-aware technology.\n",
    "\n",
    "### Tools & Libraries üõ†Ô∏è\n",
    "\n",
    "We will be using the below tools and libriaries for our implementation\n",
    "- TensorFlow: An open-source machine learning framework that includes tools for building and training deep learning models.\n",
    "- Keras: A high-level neural network API that runs on top of TensorFlow, making it easier to build and experiment with deep learning models.\n",
    "- Matplotlib: A visualisation library üìäüêç\n",
    "\n",
    "## Implementation Steps\n",
    "1. **Environment setup:** \n",
    "2. **Import necessary libraries:**\n",
    "3. **Dataset collection:**\n",
    "4. **Data Augmentation:**\n",
    "5. **CNN model building:**\n",
    "6. **Model training:**\n",
    "7. **Model evaluation:**\n",
    "\n",
    "Throughout the notebook, we'll provide detailed explanations, code snippets and visualizations to ensure a clear understanding of the entire process. üß†üõ†Ô∏è\n",
    "\n",
    "Let's get started! üöÄüöÄüöÄüöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf2718",
   "metadata": {},
   "source": [
    "## Step 1: Environment setup\n",
    "Ensure all necessary libraries are installed. Installation can be achieved using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pip to install the necessary libraries\n",
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install opencv-python\n",
    "\n",
    "\n",
    "# The below code snippet is used to check the versions of the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import cv2\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9be98f",
   "metadata": {},
   "source": [
    "## Step 2: Import necessary libraries\n",
    "\n",
    "Import all required libraries in this section of your notebook. It is always good practice to keep your code tidy and well structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75f282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\danny\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# operating system libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Open Computer vision libraries\n",
    "import cv2 as cv\n",
    "\n",
    "# File operation libraries\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd8eb1",
   "metadata": {},
   "source": [
    "## Step 3: Dataset collection\n",
    "\n",
    "For the purpose this exercise, we will use facial expression dataset named **[AffectNeT](https://www.kaggle.com/datasets/fatihkgg/affectnet-yolo-format/download?datasetVersionNumber=2)** from Kaggle. \n",
    "The **[AffectNeT](https://www.kaggle.com/datasets/fatihkgg/affectnet-yolo-format/download?datasetVersionNumber=2)** dataset is intended for use in YOLO projects involving facial expression recognition.\n",
    "\n",
    "According to the instructions on the Kaggle website, the entire data set is separated into train-test-validation folders with modified image names and matching txt files with annotations. All images are exactly 96 x 96 pixels.\n",
    "\n",
    "The dataset is shuffled and divided into three parts:\n",
    "\n",
    "- train (0.7)\n",
    "- validation (0.2)\n",
    "- test (0.1)\n",
    "\n",
    "\n",
    "There are 8 emotion classes which are mapped as:\n",
    "- 0- Anger\n",
    "- 1- Contempt\n",
    "- 2- Disgust\n",
    "- 3- Fear\n",
    "- 4- Happy\n",
    "- 5- Neutral\n",
    "- 6- Sad\n",
    "- 7- Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2350bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "dataset_dir = \"dataset/YOLO_format\"\n",
    "\n",
    "# Define subdirectories for test, train, validation sets\n",
    "train_dir = os.path.join(dataset_dir, \"train\", \"images\")\n",
    "valid_dir = os.path.join(dataset_dir, \"valid\", \"images\")\n",
    "test_dir = os.path.join(dataset_dir, \"test\", \"images\")\n",
    "\n",
    "# Define label directory for train set\n",
    "train_label_dir = os.path.join(dataset_dir, \"train\", \"labels\")\n",
    "\n",
    "# Lets try and list the files in each directory\n",
    "train_images = os.listdir(train_dir)\n",
    "valid_images = os.listdir(valid_dir)\n",
    "test_images = os.listdir(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67682ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to display some sample images from each directory\n",
    "def display_images(directory, images):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Display 9 random images\n",
    "    random_images = random.sample(images, 9)\n",
    "    \n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        img_path = os.path.join(directory, random_images[i])\n",
    "        \n",
    "        # Read image using cv.imread\n",
    "        img = cv.imread(img_path)\n",
    "        \n",
    "        # OpenCV loads images in BGR, thus we need to convert to RGB\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Plot image using matplotlib\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(images[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc0e18",
   "metadata": {},
   "source": [
    "### Image Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display train set images\n",
    "display_images(train_dir, train_images)\n",
    "plt.suptitle('Sample Images from Train Set')\n",
    "plt.show()\n",
    "\n",
    "# Display validation set images\n",
    "display_images(valid_dir, valid_images)\n",
    "plt.suptitle('Sample Images from Validation Set')\n",
    "plt.show()\n",
    "\n",
    "# Display Test set images\n",
    "display_images(test_dir, test_images)\n",
    "plt.suptitle('Sample Images from Test Set')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aabbdf",
   "metadata": {},
   "source": [
    "### 3.1 Organize YOLO_format dataset into their respective classes as defined in the data.yaml\n",
    "The original data.yaml has been modified to reflect my directory paths for the extracted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76075116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a couple of functions to carry out this activity.\n",
    "\n",
    "def create_yolo_datasets(data_path, output_path):\n",
    "    with open(data_path, 'r') as yaml_file:\n",
    "        data = yaml.safe_load(yaml_file)\n",
    "        \n",
    "        # Define paths\n",
    "        train_path = os.path.join(output_path, 'train')\n",
    "        val_path = os.path.join(output_path, 'val')\n",
    "        test_path = os.path.join(output_path, 'test')\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(train_path, exist_ok=True)\n",
    "        os.makedirs(val_path, exist_ok=True)\n",
    "        os.makedirs(test_path, exist_ok=True)\n",
    "        \n",
    "        # Organize images into subdirectories based on class names\n",
    "        organize_images_by_class(data['train'], train_path)\n",
    "        organize_images_by_class(data['val'], val_path)\n",
    "        organize_images_by_class(data['test'], test_path)\n",
    "        \n",
    "        # Create class names file\n",
    "        create_classes_file(data['names'], output_path)\n",
    "        \n",
    "def copy_images(image_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(image_dir)\n",
    "    for image in images:\n",
    "        image_path = os.path.join(image_dir, image)\n",
    "        shutil.copy(image_path, output_dir)\n",
    "        \n",
    "def create_classes_file(class_names, output_path):\n",
    "    classess_file_path = os.path.join(output_path, 'classes.txt')\n",
    "    with open(classess_file_path, 'w') as classes_file:\n",
    "        for class_name in class_names:\n",
    "            classes_file.write(f'{class_name}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b220bbe",
   "metadata": {},
   "source": [
    "### Build YOLO datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f305db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_yaml_path = \"dataset/YOLO_format/data.yaml\"\n",
    "output_dataset_path = \"dataset/yolo_dataset\"\n",
    "\n",
    "# Build YOLO datasets\n",
    "create_yolo_datasets(data_yaml_path, output_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0b804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe822c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0948acdc",
   "metadata": {},
   "source": [
    "## Step 4: Data Augmentation:\n",
    "The data augmentation helps to increase the diversity of the training dataset, thus improving the generalization of the model. TensorFlow provides\n",
    "\n",
    "The images have different backgrounds which can interfere with the model performance. We need to focus only on the Region Of Interest (ROI) which is the face.\n",
    "\n",
    "We will use pre-trained **[Haar-cascade](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)** face classifier from OpenCV for face detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37da8",
   "metadata": {},
   "source": [
    "### Remove noisy background\n",
    "\n",
    "The below code is an adaptation from the tutorial code on **[Haar-cascade](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)** website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Haar-cascade face classifier from OpenCV\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# Function to extract face from iamge\n",
    "def extract_face(img_path):\n",
    "    img = cv.imread(img_path)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect face in the image\n",
    "    face = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=2)\n",
    "    \n",
    "    # Extract ROI for each face\n",
    "    extracted_face = [img[y:y+h, x:x+w] for (x, y, w, h) in face]\n",
    "    \n",
    "    return extracted_face\n",
    "\n",
    "# Function to display sample images with extracted face.\n",
    "def display_image_with_face(directory, images):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Display 9 random images\n",
    "    random_images = random.sample(images, 9)\n",
    "    \n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        img_path = os.path.join(directory, random_images[i])\n",
    "        extracted_face = extract_face(img_path)\n",
    "        \n",
    "        if extracted_face:\n",
    "            # Display the first extracted face\n",
    "            plt.imshow(cv.cvtColor(extracted_face[0], cv.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.title(random_images[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e231a03",
   "metadata": {},
   "source": [
    "### Image Display of Extracted face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display train set images with extracted faces\n",
    "display_image_with_face(train_dir, train_images)\n",
    "plt.suptitle('Sample Images from Train Set with Extracted Faces')\n",
    "plt.show()\n",
    "\n",
    "# Display validation set images with extracted faces\n",
    "display_image_with_face(valid_dir, valid_images)\n",
    "plt.suptitle('Sample Images from Validation Set with Extracted Faces')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b8dff",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbcd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation parameters for training sets\n",
    "train_datagen = ImageDataGenerator (\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data augmentation for validation and test sets (usually no augmentation is necessary for these data sets)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess a sample image for demonstration\n",
    "sample_img_path = os.path.join(train_dir, train_images[0])\n",
    "sample_img = extract_face(sample_img_path)\n",
    "sample_img = cv.cvtColor(sample_img[0], cv.COLOR_BGR2RGB)\n",
    "sample_img = sample_img.reshape((1,) + sample_img.shape) # Reshape to (1, height, width, channels)\n",
    "\n",
    "# Display original and augmented images for comparison\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(sample_img[0])\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Augmented image\n",
    "for i, augmented_img in enumerate(train_datagen.flow(sample_img, batch_size=1)):\n",
    "    plt.subplot(1, 4, i + 2)\n",
    "    plt.imshow(augmented_img[0])\n",
    "    plt.title(f'Augmented Image {i + 1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Lets Display three augmented images for demonstration\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daebe72",
   "metadata": {},
   "source": [
    "### Image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator for training, validation, and test sets\n",
    "\n",
    "image_size = (96, 96) # We maintain original sizes of images.\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True # Enable shuffle to make model more robust\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21526497",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ba872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753e662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02989c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd538fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3080ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
